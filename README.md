# PL-Repo.

114-1 å¸«å¤§ç§‘æŠ€ç³»ç¨‹å¼èªè¨€
 - æˆèª²æ•™å¸«ï¼šè”¡èŠ¸ç¤è€å¸«
 - å§“åï¼šè¶™å˜‰é‹
 - ç³»ç´šï¼šç§‘æŠ€ç³»äºŒå¹´ç´š
 - èª²ç¨‹ç­†è¨˜å€
   ---
 - ä½œæ¥­é€£çµå€
   ---
   - [HW1](https://github.com/icecat14159/PL-Repo./blob/main/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80HW01_%E6%97%A5%E5%B8%B8%E6%94%AF%E5%87%BA%E9%80%9F%E7%AE%97%E8%88%87%E5%88%86%E6%94%A4.ipynb)
   - [HW2](https://github.com/icecat14159/PL-Repo./blob/main/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80HW02_%E6%88%90%E7%B8%BE%E4%B8%80%E6%9C%AC%E9%80%9A.ipynb)(10/5 æ™š20:00æ¸¬è©¦æ™‚åŸæ¨¡å‹gemini-1.5-flash-latestä»å¯ç”¨(ç¬¬å››æ®µåŸ·è¡Œæ­£å¸¸)ï¼Œ10/6æ—©9:30å†æ¬¡æ¸¬è©¦æ™‚å·²ä¸å¯ç”¨(ç¬¬å››æ®µå‡ºç¾éŒ¯èª¤)ï¼Œæ•…æ”¹ç‚ºgemini-flash-latestï¼Œä½†ä»ç„¶è¶…æ™‚)
   - [HW3](https://github.com/icecat14159/PL-Repo./blob/main/HW03_%E6%9B%B8%E7%B1%8D%E6%B8%85%E5%96%AE.ipynb)ï¼Œ[åŒ¯å‡ºçš„csvæª”æ¡ˆ](https://github.com/icecat14159/PL-Repo./blob/main/books.csv)
   - [HW4](https://github.com/icecat14159/PL-Repo./blob/main/HW04_%E7%B6%B2%E9%A0%81%E7%88%AC%E8%9F%B2.ipynb)(åŸå…ˆæŠ“å–æˆåŠŸï¼Œè©¦ç®—è¡¨ä¹Ÿæœ‰å…§å®¹ã€‚å¾Œä¾†ç¬¬ä¸€æ®µå†æ¬¡æ¸¬è©¦æ™‚å‡ºç¾ç‹€æ…‹ç¢¼429ï¼Œæ•…é¡¯ç¤ºç‚ºæŠ“å–0ç­†æ–‡ç« )
   - [HW5](https://github.com/icecat14159/PL-Repo./blob/main/HW05%E5%B0%8B%E6%89%BE%E8%94%AC%E9%A3%9F%E9%A4%90%E5%BB%B3.ipynb)
   - [HW6](https://github.com/icecat14159/PL-Repo./blob/main/HW06_%E6%9B%B8%E7%B1%8D%E6%B8%85%E5%96%AE2.ipynb)
 - å°ˆé¡Œé€£çµå€
   ---
   - [ææ¡ˆç°¡å ±](https://www.canva.com/design/DAG5JeIAvPs/LlJdgvb4HQ7s0yHEz_eHrw/edit?utm_content=DAG5JeIAvPs&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)
   - [ææ¡ˆå½±ç‰‡](https://youtu.be/LoHRQU6lJt4)
   - [å°ˆæ¡ˆç¨‹å¼](è¿½ç•ªæ¸…å–®.ipynb)

  ğŸ“º è¿½ç•ªæ¸…å–®ç®¡ç†ç³»çµ±ï¼ˆGradio + Google Sheetï¼‰
  
  æœ¬å°ˆæ¡ˆæä¾›ä¸€å€‹åŸºæ–¼ Gradio çš„ç¶²é ä»‹é¢ï¼Œç”¨æ–¼çµ±ä¸€ç®¡ç†
  å°èªªã€æ¼«ç•«ã€å‹•ç•«çš„è¿½è¹¤æ¸…å–®ï¼Œä¸¦å°‡è³‡æ–™å„²å­˜æ–¼ Google Sheetã€‚
  
  --------------------------------------------------
  ğŸš€ åŠŸèƒ½ç‰¹è‰²
  --------------------------------------------------
  - å°èªªï¼æ¼«ç•«ï¼å‹•ç•«ä¸‰ç¨®åª’é«”åˆ†é–‹ç®¡ç†
  - å¯ç›´æ¥åœ¨è¡¨æ ¼ä¸­ç·¨è¼¯ä½œå“è³‡è¨Š
  - ä¸€éµåŒæ­¥è‡³ Google Sheet
  - é•·æ™‚é–“æœªæ›´æ–°ä½œå“è‡ªå‹•ç¤ºè­¦
  - è·¨åª’é«”åŒåä½œå“ ID å°æ‡‰é¡¯ç¤º
  - ä¾æ›´æ–°ç¶²å€è‡ªå‹•æª¢æŸ¥é€²åº¦
  - è·¨åª’é«”æœå°‹èˆ‡ç¯©é¸åŠŸèƒ½
  --------------------------------------------------
  ç¨‹å¼ç‰‡æ®µèªªæ˜
  --------------------------------------------------
### ç¬¬ä¸€æ­¥:å®‰è£…ä¾é™„å…ƒä»¶
```bash
!pip install gspread gradio pandas beautifulsoup4 requests
from google.colab import auth #èº«åˆ†é©—è­‰
from google.auth import default #æ†‘è­‰
import gspread
import gradio as gr
import pandas as pd
import datetime #æ—¥æœŸ
import io #io
import requests
from bs4 import BeautifulSoup
import re
!pip install feedparser
import feedparser
```
### ç¬¬äºŒæ­¥:æˆæ¬Šé€£ç·šã€é€£çµè©¦ç®—è¡¨
```bash
# æˆæ¬Šé€£ç·š Google Sheet
auth.authenticate_user()  #è¦æ±‚æˆæ¬Š
creds, _ = default()  #ç²å–æ†‘è­‰
gc = gspread.authorize(creds) #æˆæ¬Š
# é€£çµè©¦ç®—è¡¨
sheet_url = "https://docs.google.com/spreadsheets/d/1fGjPVqPHt3flo-LxBNU9EZl_ZMvg-I8UXfMhIY-jDNA/edit?usp=sharing"  #GoogleSheeté€£çµ
sh = gc.open_by_url(sheet_url)  #é€²å…¥è©¦ç®—è¡¨
WORKSHEETS = {
    "novel": sh.worksheet("å°èªªæ¸…å–®"),
    "comic": sh.worksheet("æ¼«ç•«æ¸…å–®"),
    "anime": sh.worksheet("å‹•ç•«æ¸…å–®")
}
SNAPSHOT_SHEET = sh.worksheet("update_snapshot")
```
### ç¬¬ä¸‰æ­¥:è®€å–ã€å¯«å›è©¦ç®—è¡¨
```bash
# è®€å–è©¦ç®—è¡¨
COLUMNS = ["ID", "ä½œå“åç¨±", "ä½œè€…", "è©•ç´š", "é€²åº¦", "ç‹€æ…‹", "æœ€å¾Œç´€éŒ„æ—¥æœŸ", "æ›´æ–°ç¶²å€"]  #è³‡æ–™åº«8æ¬„
DISPLAY_COLUMNS = COLUMNS + ["è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)"] #é¡¯ç¤ºç”¨8æ¬„

def R_D(media):
    ws = WORKSHEETS[media]
    records = ws.get_all_records()
    df = pd.DataFrame(records)

    if df.empty:
        df = pd.DataFrame(columns=COLUMNS)

    df["ID"] = pd.to_numeric(df["ID"], errors="coerce")

    def calc_days(d):
        try:
            return (datetime.date.today() - datetime.datetime.strptime(d, "%Y/%m/%d").date()).days
        except:
            return None

    df["è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)"] = df["æœ€å¾Œç´€éŒ„æ—¥æœŸ"].apply(calc_days)
    return df

def read_data(media):
  worksheet = WORKSHEETS[media]
  records = worksheet.get_all_records()
  df = pd.DataFrame(records)

  if df.empty:
    df = pd.DataFrame(columns=COLUMNS)

  #ç¢ºä¿ID
  if "ID" in df.columns:
    df["ID"] = pd.to_numeric(df["ID"], errors="coerce")

  #è¨ˆç®—è·é›¢ä¸Šæ¬¡ç´€éŒ„æ™‚é–“
  today = datetime.date.today()

  def calc_days(date_str):
    try:
      last_date = datetime.datetime.strptime(date_str, "%Y/%m/%d").date()
      return (today - last_date).days
    except:
      return None
  df["è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)"] = df["æœ€å¾Œç´€éŒ„æ—¥æœŸ"].apply(calc_days)

  return df
# å¯«å›è©¦ç®—è¡¨
def write_data(df, media):
  worksheet = WORKSHEETS[media]
  df = df[COLUMNS] #å¼·åˆ¶åªç•™ä¸‹è³‡æ–™åº«æ¬„ä½
  worksheet.clear()
  worksheet.append_row(COLUMNS)
  worksheet.append_rows(df.values.tolist())
```
### ç¬¬å››æ­¥:æ–°å¢ã€ä¿®æ”¹ã€åˆªé™¤ç´€éŒ„
```bash
# æ–°å¢ç´€éŒ„
def add_record(name, author, rating, progress, condition, date, media):
  worksheet = WORKSHEETS[media]
  df = read_data(media)
  if not date:
      date = datetime.date.today().strftime("%Y/%m/%d")
  if df.empty or df["ID"].dropna().empty: #ç”¢ç”ŸID
    new_id = 1
  else:
    new_id = int(df["ID"].max()) + 1
  new_entry = {"ID": new_id, "ä½œå“åç¨±": name, "ä½œè€…": author, "è©•ç´š": rating, "é€²åº¦": progress, "ç‹€æ…‹": condition, "æœ€å¾Œç´€éŒ„æ—¥æœŸ": date}
  df = pd.concat([df, pd.DataFrame([new_entry])], ignore_index=True)
  write_data(df, media)
  return df
# ä¿®æ”¹ç´€éŒ„
def edit_record(record_id, name, author, rating, progress, condition, date, media):
  worksheet = WORKSHEETS[media]
  df = read_data(media)
  if record_id not in df["ID"].values:
    return "æ‰¾ä¸åˆ°æ­¤ ID"
  if not date:
    date = datetime.date.today().strftime("%Y/%m/%d")

  df.loc[df["ID"] == record_id, ["ä½œå“åç¨±", "ä½œè€…", "è©•ç´š", "é€²åº¦", "ç‹€æ…‹", "æœ€å¾Œç´€éŒ„æ—¥æœŸ"]] = \
      [name, author, rating, progress, condition, date]
  write_data(df, media)
  return df
# åˆªé™¤ç´€éŒ„
def delete_record(record_id, media):
  worksheet = WORKSHEETS[media]
  df = read_data()
  if record_id not in df["ID"].values:
    return "æ‰¾ä¸åˆ°æ­¤ ID"
  df = df[df["ID"] != record_id]
  write_data(df, media)
  return df
```
### ç¬¬äº”æ­¥:ä»¥Gradioæ”¹å‹•è¡¨å–®
```bash
# gradioç”¨ æ”¹å‹•è¡¨å–®
def save_table(table_df, media):
  worksheet = WORKSHEETS[media]
  df = pd.DataFrame(table_df, columns=DISPLAY_COLUMNS) # è½‰æˆDataFrameä¸¦å¼·åˆ¶æ¬„ä½é †åº
  df = df[COLUMNS] #ä¸Ÿæ‰ç¬¬8æ¬„
  df["ID"] = range(1, len(df) + 1) #ä¿®æ­£IDï¼ˆé˜²æ­¢ä½¿ç”¨è€…äº‚æ”¹/åˆªåˆ—ï¼‰

  today = datetime.date.today().strftime("%Y/%m/%d")
  df["æœ€å¾Œç´€éŒ„æ—¥æœŸ"] = df["æœ€å¾Œç´€éŒ„æ—¥æœŸ"].replace("", today)
  write_data(df, media)

  return read_data(media)
# gradioç”¨ æ–°å¢è³‡æ–™åˆ—
def add_empty_row(table_df, media):
  worksheet = WORKSHEETS[media]
  df = pd.DataFrame(table_df, columns=DISPLAY_COLUMNS)

  new_row = {
    "ID": "",
    "ä½œå“åç¨±": "",
    "ä½œè€…": "",
    "è©•ç´š": "",
    "é€²åº¦": "",
    "ç‹€æ…‹": "",
    "æœ€å¾Œç´€éŒ„æ—¥æœŸ": "",
    "è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)": ""
  }

  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
  return df
def delete_and_refresh(record_id, media):
  if record_id is None:
    return read_data(media)
  try:
    record_id = int(record_id)
  except:
    return read_data(media)
  df = read_data(media)
  if record_id not in df["ID"].values:
    return df
  # åŸ·è¡Œåˆªé™¤
  df = df[df["ID"] != record_id].reset_index(drop=True)
  if df.empty:
    return df
  df["ID"] = range(1, len(df) + 1)
  write_data(df, media)
  return read_data(media)
```
### ç¬¬å…­æ­¥:ç¤ºè­¦æ¸…å–®
```bash
ALERT_RULES = {
  "novel": 90, #å°èªª3å€‹æœˆ
  "comic": 30, #æ¼«ç•«1å€‹æœˆ
  "anime": 14 #å‹•ç•«14å¤©
}
ALERT_COLUMNS = [
    "ID",
    "ä½œå“åç¨±",
    "ä½œè€…",
    "é€²åº¦",
    "ç‹€æ…‹",
    "æœ€å¾Œç´€éŒ„æ—¥æœŸ",
    "è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)"
]

def get_alert_table(media):
  df = read_data(media)
  if df.empty:
      return df
  threshold = ALERT_RULES[media]
  alert_df = df[
      (df["ç‹€æ…‹"] == "æœªå®Œçµ") &
      (df["è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)"].notna()) &
      (df["è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)"] >= threshold)
  ]
  return alert_df.reset_index(drop=True)
```
### ç¬¬ä¸ƒæ­¥:è·¨åª’é«”åµæ¸¬
```bash
CROSS_MEDIA_CONFIG = {
    "novel": {
        "targets": ["comic", "anime"],
        "columns": ["å°æ‡‰æ¼«ç•«ID", "å°æ‡‰å‹•ç•«ID"]
    },
    "comic": {
        "targets": ["novel", "anime"],
        "columns": ["å°æ‡‰å°èªªID", "å°æ‡‰å‹•ç•«ID"]
    },
    "anime": {
        "targets": ["novel", "comic"],
        "columns": ["å°æ‡‰å°èªªID", "å°æ‡‰æ¼«ç•«ID"]
    }
}
def get_cross_columns(media):
  return ["ID", "ä½œå“åç¨±"] + CROSS_MEDIA_CONFIG[media]["columns"]

def build_cross_media_table(media):
  base_df = read_data(media)

  if base_df.empty:
      return pd.DataFrame(columns=get_cross_columns(media))

  targets = CROSS_MEDIA_CONFIG[media]["targets"]
  target_dfs = {t: read_data(t) for t in targets}

  rows = []

  for _, row in base_df.iterrows():
      title = row["ä½œå“åç¨±"]
      base_id = int(row["ID"])

      cross_ids = {}

      for t in targets:
          match = target_dfs[t][target_dfs[t]["ä½œå“åç¨±"] == title]
          cross_ids[t] = int(match.iloc[0]["ID"]) if not match.empty else ""

      # è‡³å°‘æœ‰ä¸€å€‹å°æ‡‰æ‰é¡¯ç¤º
      if any(cross_ids.values()):
          entry = {
              "ID": base_id,
              "ä½œå“åç¨±": title
          }

          for t, col_name in zip(targets, CROSS_MEDIA_CONFIG[media]["columns"]):
              entry[col_name] = cross_ids[t]

          rows.append(entry)

  return pd.DataFrame(rows, columns=get_cross_columns(media))
```
### ç¬¬å…«æ­¥:æŸ¥è©¢ç³»çµ±
```bash
SEARCH_COLUMNS = [
    "ID", "ä½œå“åç¨±", "ä½œè€…", "è©•ç´š", "é€²åº¦",
    "ç‹€æ…‹", "æœ€å¾Œç´€éŒ„æ—¥æœŸ", "è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)", "é¡å‹"
]
def get_all_media_data():
  dfs = []

  for media, label in [
      ("novel", "å°èªª"),
      ("comic", "æ¼«ç•«"),
      ("anime", "å‹•ç•«")
  ]:
    df = read_data(media)
    if df.empty:
        continue

    df = df.copy()
    df["é¡å‹"] = label
    dfs.append(df)

  if not dfs:
    return pd.DataFrame(columns=SEARCH_COLUMNS)

  return pd.concat(dfs, ignore_index=True)

def search_works(keyword, rating, status, min_days):
  df = get_all_media_data()

  if df.empty:
      return df

  # é—œéµå­—æœå°‹ï¼ˆID / ä½œå“ / ä½œè€…ï¼‰
  if keyword:
    keyword = str(keyword).strip()
    df = df[
      df["ä½œå“åç¨±"].astype(str).str.contains(keyword, case=False, na=False) |
      df["ä½œè€…"].astype(str).str.contains(keyword, case=False, na=False) |
      df["ID"].astype(str).str.contains(keyword, na=False)
    ]

  # è©•ç´šç¯©é¸
  if rating != "å…¨éƒ¨":
    df = df[df["è©•ç´š"] == rating]

  # ç‹€æ…‹ç¯©é¸
  if status != "å…¨éƒ¨":
    df = df[df["ç‹€æ…‹"] == status]

  # è·é›¢ä¸Šæ¬¡ç´€éŒ„
  if min_days is not None:
    df = df[df["è·é›¢ä¸Šæ¬¡ç´€éŒ„(å¤©)"] >= min_days]

  return df[SEARCH_COLUMNS]
```
### ç¬¬ä¹æ­¥:æ§‹å»ºç´€éŒ„æ›´æ–°ç”¨çš„snapshot
```bash
# ---------- Snapshot ----------
def load_snapshot():
    records = SNAPSHOT_SHEET.get_all_records()
    if not records:
        return pd.DataFrame(columns=["åª’é«”", "ID", "ä½œå“åç¨±", "é€²åº¦"])
    return pd.DataFrame(records)


def save_snapshot(df):
    SNAPSHOT_SHEET.clear()
    SNAPSHOT_SHEET.append_row(["åª’é«”", "ID", "ä½œå“åç¨±", "é€²åº¦"])
    if not df.empty:
        SNAPSHOT_SHEET.append_rows(df.values.tolist())
```
### ç¬¬åæ­¥:æ§‹å»ºç”¨ä¾†è’é›†è³‡æ–™çš„parsers
```bash

def parse_lnovel(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")

        volumes = []

        for text in soup.stripped_strings:
            text = text.strip()

            # âœ… æ ¸å¿ƒï¼šæŠ“ã€Œç¬¬Xå·ã€ï¼ŒX å¯ç‚ºä¸­æ–‡æˆ–æ•¸å­—
            m = re.search(r"ç¬¬\s*([é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)\s*å·", text)
            if m:
                raw = m.group(1)
                num = chinese_to_int(raw)
                if num > 0:
                    volumes.append(num)

        if not volumes:
            return "æœªçŸ¥"

        latest = max(volumes)
        return f"ç¬¬{latest}å·"

    except Exception:
        return "æœªçŸ¥"

def parse_manhuagui(url):
    try:
        soup = BeautifulSoup(requests.get(url, timeout=10).text, "html.parser")
        chapters = soup.select(".chapter-list a")
        if chapters:
            return chapters[0].text.strip()
        return "æœªçŸ¥"
    except:
        return "æœªçŸ¥"

def parse_bahamut(url):
    return "è«‹æ‰‹å‹•æ›´æ–°"

def parse_update_date(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        html = requests.get(url, headers=headers, timeout=10).text
        soup = BeautifulSoup(html, "html.parser")

        # å¸¸è¦‹æ›´æ–°æ—¥æœŸæ–‡å­—
        for text in soup.stripped_strings:
            if "æ›´æ–°" in text and any(c.isdigit() for c in text):
                return text.strip()

        return "æœªçŸ¥"
    except:
        return "æœªçŸ¥"

def parse_generic(url):
    try:
        soup = BeautifulSoup(requests.get(url, timeout=10).text, "html.parser")
        for t in soup.stripped_strings:
            if any(k in t.lower() for k in ["è©±", "é›†", "ç« ", "ep", "episode"]) and len(t) <= 15:
                return t.strip()
        return "æœªçŸ¥"
    except:
        return "æœªçŸ¥"

PARSERS = {
    "manhuagui.com": parse_manhuagui,
    "www.manhuagui.com": parse_manhuagui,
    "ani.gamer.com.tw": parse_bahamut,
    "lnovel.tw": parse_lnovel,
    "www.lnovel.tw": parse_lnovel,
}


def get_domain(url):
    try:
        return url.split("//")[1].split("/")[0]
    except:
        return ""


def get_latest_from_url(url, debug=True):
    domain = get_domain(url)

    if debug:
        print(f"[DEBUG] URL = {url}")
        print(f"[DEBUG] domain = {domain}")

    parser = PARSERS.get(domain)

    if not parser:
        if debug:
            print("[DEBUG] âŒ No parser found for this domain")
        return "æœªçŸ¥"

    if debug:
        print(f"[DEBUG] âœ… Using parser: {parser.__name__}")

    try:
        result = parser(url)

        if debug:
            print(f"[DEBUG] ğŸ“¤ Parser result = {result}")

        return result
    except Exception as e:
        if debug:
            print(f"[DEBUG] ğŸ’¥ Parser exception: {e}")
        return "æœªçŸ¥"

```
### ç¬¬åä¸€æ­¥:å»ºç«‹æª¢æ¸¬æ›´æ–°çš„æ©Ÿåˆ¶
```bash
# ---------- æ›´æ–°æª¢æŸ¥ ----------
def detect_updates(media):
    df = R_D(media)
    snapshot = load_snapshot()
    alerts = []
    new_rows = []

    for _, r in df.iterrows():
        cid = str(r["ID"])
        title = r["ä½œå“åç¨±"]
        latest = get_latest_from_url(r.get("æ›´æ–°ç¶²å€", ""))

        snap = snapshot[(snapshot["åª’é«”"] == media) & (snapshot["ID"] == cid)]
        old = r["é€²åº¦"]

        if latest != "æœªçŸ¥" and latest != old:
            alerts.append(f"{title}ï¼š{old} â†’ {latest}")
            df.loc[df["ID"] == int(cid), "é€²åº¦"] = latest
            df.loc[df["ID"] == int(cid), "æœ€å¾Œç´€éŒ„æ—¥æœŸ"] = TD()
            new_rows.append([media, cid, title, latest])
        else:
            new_rows.append([media, cid, title, old])

    #W_D(df, media)
    save_snapshot(pd.concat([
        snapshot[snapshot["åª’é«”"] != media],
        pd.DataFrame(new_rows, columns=["åª’é«”", "ID", "ä½œå“åç¨±", "é€²åº¦"])
    ], ignore_index=True))

    return alerts
def get_gradio_alerts():
    msgs = []
    for m, label in [("novel", "å°èªª"), ("comic", "æ¼«ç•«"), ("anime", "å‹•ç•«")]:
        for a in detect_updates(m):
            msgs.append(f"- {label}ï½œ{a}")
    return "âœ… ç›®å‰æ²’æœ‰æ›´æ–°" if not msgs else "ğŸ“¢ **æ›´æ–°æª¢æŸ¥çµæœ**\n\n" + "\n".join(msgs)
```
### ç¬¬åäºŒæ­¥:Gradioæ“ä½œä»‹é¢
```bash
import gradio as gr

#gradioæ“ä½œä»‹é¢
with gr.Blocks() as app:
  gr.Markdown("è¿½ç•ªæ¸…å–®")
  with gr.Tab("å°èªªæ¸…å–®"): #åˆ†é 
    gr.Markdown("ç›´æ¥ç·¨è¼¯æ¸…å–®å¾Œé»æ“Šå„²å­˜æ›´è®Šå³å¯å„²å­˜!")
    refresh_novel = gr.Button("é‡æ–°æ•´ç†") #åˆ·æ–°æŒ‰éˆ•
    with gr.Row():
      add_novel = gr.Button("æ–°å¢ä¸€ç­†")
      del_novel_id = gr.Number(label="åˆªé™¤ ID", precision=0)
      del_novel = gr.Button("åˆªé™¤è©²ç­†", variant="stop")
    save_novel = gr.Button("å„²å­˜è®Šæ›´", variant="secondary")

    table_novel = gr.Dataframe( #è¡¨å–®
      value=read_data("novel"),
      headers=COLUMNS,
      interactive=True
    )

    gr.Markdown("ç³»çµ±åµæ¸¬åˆ°ä»¥ä¸‹ä½œå“å­˜åœ¨è·¨åª’é«”")
    cross_table_novel = gr.Dataframe(
        value=build_cross_media_table("novel"),
        headers=get_cross_columns("novel"),
        interactive=False
    )

    refresh_novel.click(fn=lambda: read_data("novel"), outputs=table_novel)
    add_novel.click(
        fn=lambda table: add_empty_row(table, "novel"),
        inputs=table_novel,
        outputs=table_novel
    )
    del_novel.click(
      fn=lambda rid: delete_and_refresh(rid, "novel"),
      inputs=del_novel_id,
      outputs=table_novel
    )

    save_novel.click(
        fn=lambda table: save_table(table, "novel"),
        inputs=table_novel,
        outputs=table_novel
    )

  with gr.Tab("æ¼«ç•«æ¸…å–®"):
    gr.Markdown("ç›´æ¥ç·¨è¼¯æ¸…å–®å¾Œé»æ“Šå„²å­˜æ›´è®Šå³å¯å„²å­˜!")
    refresh_comic = gr.Button("é‡æ–°æ•´ç†") #åˆ·æ–°æŒ‰éˆ•
    with gr.Row():
      add_comic = gr.Button("æ–°å¢ä¸€ç­†")
      del_comic_id = gr.Number(label="åˆªé™¤ ID", precision=0)
      del_comic = gr.Button("åˆªé™¤è©²ç­†", variant="stop")
    save_comic = gr.Button("å„²å­˜è®Šæ›´", variant="secondary")

    table_comic = gr.Dataframe( #è¡¨å–®
      value=read_data("comic"),
      headers=COLUMNS,
      interactive=True
    )

    gr.Markdown("ç³»çµ±åµæ¸¬åˆ°ä»¥ä¸‹ä½œå“å­˜åœ¨è·¨åª’é«”")
    cross_table_comic = gr.Dataframe(
        value=build_cross_media_table("comic"),
        headers=get_cross_columns("comic"),
        interactive=False
    )

    refresh_comic.click(fn=lambda: read_data("comic"), outputs=table_comic)
    add_comic.click(
        fn=lambda table: add_empty_row(table, "comic"),
        inputs=table_comic,
        outputs=table_comic
    )
    del_comic.click(
      fn=lambda rid: delete_and_refresh(rid, "comic"),
      inputs=del_comic_id,
      outputs=table_comic
    )

    save_comic.click(
        fn=lambda table: save_table(table, "comic"),
        inputs=table_comic,
        outputs=table_comic
    )

  with gr.Tab("å‹•ç•«æ¸…å–®"):
    gr.Markdown("ç›´æ¥ç·¨è¼¯æ¸…å–®å¾Œé»æ“Šå„²å­˜æ›´è®Šå³å¯å„²å­˜!")
    refresh_anime = gr.Button("é‡æ–°æ•´ç†") #åˆ·æ–°æŒ‰éˆ•
    with gr.Row():
      add_anime = gr.Button("æ–°å¢ä¸€ç­†")
      del_anime_id = gr.Number(label="åˆªé™¤ ID", precision=0)
      del_anime = gr.Button("åˆªé™¤è©²ç­†", variant="stop")
    save_anime = gr.Button("å„²å­˜è®Šæ›´", variant="secondary")

    table_anime = gr.Dataframe( #è¡¨å–®
      value=read_data("anime"),
      headers=COLUMNS,
      interactive=True
    )

    gr.Markdown("ç³»çµ±åµæ¸¬åˆ°ä»¥ä¸‹ä½œå“å­˜åœ¨è·¨åª’é«”")
    cross_table_anime = gr.Dataframe(
        value=build_cross_media_table("anime"),
        headers=get_cross_columns("anime"),
        interactive=False
    )

    refresh_anime.click(fn=lambda: read_data("anime"), outputs=table_anime)
    add_anime.click(
        fn=lambda table: add_empty_row(table, "anime"),
        inputs=table_anime,
        outputs=table_anime
    )
    del_anime.click(
      fn=lambda rid: delete_and_refresh(rid, "anime"),
      inputs=del_anime_id,
      outputs=table_anime
    )

    save_anime.click(
        fn=lambda table: save_table(table, "anime"),
        inputs=table_anime,
        outputs=table_anime
    )

  with gr.Tab("ç¤ºè­¦æ¸…å–®"):
    gr.Markdown("åƒ…é¡¯ç¤ºã€Œæœªå®Œçµã€ä¸”é•·æ™‚é–“æœªæ›´æ–°çš„ä½œå“")

    alert_refresh = gr.Button("é‡æ–°æ•´ç†ç¤ºè­¦æ¸…å–®")

    gr.Markdown("å°èªªç¤ºè­¦ï¼ˆè¶…é 3 å€‹æœˆï¼‰")
    alert_novel = gr.Dataframe(
        value=get_alert_table("novel"),
        headers=ALERT_COLUMNS,
        interactive=False
    )

    gr.Markdown("æ¼«ç•«ç¤ºè­¦ï¼ˆè¶…é 1 å€‹æœˆï¼‰")
    alert_comic = gr.Dataframe(
        value=get_alert_table("comic"),
        headers=ALERT_COLUMNS,
        interactive=False
    )

    gr.Markdown("å‹•ç•«ç¤ºè­¦ï¼ˆè¶…é 14 å¤©ï¼‰")
    alert_anime = gr.Dataframe(
        value=get_alert_table("anime"),
        headers=ALERT_COLUMNS,
        interactive=False
    )

    alert_refresh.click(
      fn=lambda: (
          get_alert_table("novel"),
          get_alert_table("comic"),
          get_alert_table("anime")
      ),
      outputs=[alert_novel, alert_comic, alert_anime]
    )

  with gr.Tab("æŸ¥è©¢ç³»çµ±"):
    gr.Markdown("### è·¨åª’é«”ä½œå“æŸ¥è©¢")

    with gr.Row():
      keyword = gr.Textbox(label="é—œéµå­—ï¼ˆID / ä½œå“åç¨± / ä½œè€…ï¼‰")
      rating = gr.Dropdown(
        choices=["å…¨éƒ¨", "S", "A", "B", "C"],
        value="å…¨éƒ¨",
        label="è©•ç´š"
      )
      status = gr.Dropdown(
        choices=["å…¨éƒ¨", "æœªå®Œçµ", "å®Œçµ"],
        value="å…¨éƒ¨",
        label="ç‹€æ…‹"
      )
      min_days = gr.Number(
        label="è·é›¢ä¸Šæ¬¡ç´€éŒ„ â‰¥ N å¤©",
        precision=0
      )

    search_btn = gr.Button("æŸ¥è©¢")

    search_table = gr.Dataframe(
      headers=SEARCH_COLUMNS,
      interactive=False
    )

    search_btn.click(
      fn=search_works,
      inputs=[keyword, rating, status, min_days],
      outputs=search_table
    )
  with gr.Tab("æ›´æ–°æé†’"):
      out = gr.Markdown()
      gr.Button("æª¢æŸ¥æ›´æ–°").click(get_gradio_alerts, outputs=out)

app.launch()
```
  --------------------------------------------------
  ğŸ§­ ä»‹é¢èªªæ˜
  --------------------------------------------------
  
  ã€1ã€‘å°èªªï¼æ¼«ç•«ï¼å‹•ç•«æ¸…å–®
  - ä»¥åˆ†é é¡¯ç¤ºä¸‰ç¨®åª’é«”
  - è¡¨æ ¼å¯ç›´æ¥ç·¨è¼¯ä»¥ä¸‹æ¬„ä½ï¼š
    - ä½œå“åç¨±ã€ä½œè€…ã€è©•ç´šã€é€²åº¦ã€ç‹€æ…‹
    - æœ€å¾Œç´€éŒ„æ—¥æœŸã€æ›´æ–°ç¶²å€
  
  åŠŸèƒ½æŒ‰éˆ•ï¼š
  - é‡æ–°æ•´ç†ï¼šå¾ Google Sheet é‡æ–°è¼‰å…¥è³‡æ–™
  - æ–°å¢ä¸€ç­†ï¼šåœ¨è¡¨æ ¼åº•éƒ¨æ–°å¢ç©ºç™½åˆ—
  - åˆªé™¤è©²ç­†ï¼šè¼¸å…¥ ID å¾Œåˆªé™¤æŒ‡å®šä½œå“
  - å„²å­˜è®Šæ›´ï¼šå°‡ç›®å‰è¡¨æ ¼å…§å®¹å¯«å› Google Sheet
  
  â€» ä¿®æ”¹å¾Œå‹™å¿…é»æ“Šã€Œå„²å­˜è®Šæ›´ã€æ‰æœƒç”Ÿæ•ˆ
  
  --------------------------------------------------
  
  ã€2ã€‘è·¨åª’é«”æ¸…å–®
  - è‡ªå‹•åµæ¸¬åŒåä½œå“
  - é¡¯ç¤ºå…¶åœ¨å…¶ä»–åª’é«”ä¸­çš„å°æ‡‰ ID
  - åƒ…ä¾›æŸ¥çœ‹ï¼Œä¸å¯ç›´æ¥ç·¨è¼¯
  
  --------------------------------------------------
  
  ã€3ã€‘ç¤ºè­¦æ¸…å–®
  é¡¯ç¤ºã€Œæœªå®Œçµã€ä¸”è¶…éæŒ‡å®šå¤©æ•¸æœªæ›´æ–°çš„ä½œå“ï¼š
  - å°èªªï¼š90 å¤©
  - æ¼«ç•«ï¼š30 å¤©
  - å‹•ç•«ï¼š14 å¤©
  
  é»æ“Šã€Œé‡æ–°æ•´ç†ç¤ºè­¦æ¸…å–®ã€å¯æ›´æ–°çµæœ
  
  --------------------------------------------------
  
  ã€4ã€‘æŸ¥è©¢ç³»çµ±
  æ”¯æ´è·¨åª’é«”æœå°‹ï¼Œæ¢ä»¶åŒ…å«ï¼š
  - é—œéµå­—ï¼ˆIDï¼ä½œå“åç¨±ï¼ä½œè€…ï¼‰
  - è©•ç´šï¼ˆS / A / B / Cï¼‰
  - ç‹€æ…‹ï¼ˆå®Œçµ / æœªå®Œçµï¼‰
  - è·é›¢ä¸Šæ¬¡ç´€éŒ„å¤©æ•¸
  
  --------------------------------------------------
  
  ã€5ã€‘æ›´æ–°æé†’
  - ä¾æ“šã€Œæ›´æ–°ç¶²å€ã€è‡ªå‹•æŠ“å–æœ€æ–°é€²åº¦(ä½¿ç”¨è€…å¿…é ˆæ‰‹å‹•è¼¸å…¥)
  - è‹¥é€²åº¦æœ‰è®ŠåŒ–ï¼Œæœƒé¡¯ç¤ºæ›´æ–°æç¤º
  - è‡ªå‹•æ›´æ–°é€²åº¦èˆ‡æœ€å¾Œç´€éŒ„æ—¥æœŸ
  
  --------------------------------------------------
  ğŸ—‚ è³‡æ–™æ¬„ä½èªªæ˜
  --------------------------------------------------
  - IDï¼šç³»çµ±è‡ªå‹•ç·¨è™Ÿï¼ˆè«‹å‹¿æ‰‹å‹•èª¿æ•´ï¼‰
  - ä½œå“åç¨±ï¼šå¿…å¡«ï¼Œè·¨åª’é«”æ¯”å°ä¾æ“š
  - ä½œè€…ï¼šä½œè€…æˆ–åŸä½œ
  - è©•ç´šï¼šS / A / B / C
  - é€²åº¦ï¼šç›®å‰è§€çœ‹ï¼é–±è®€é€²åº¦
  - ç‹€æ…‹ï¼šæœªå®Œçµ / å®Œçµ
  - æœ€å¾Œç´€éŒ„æ—¥æœŸï¼šæœ€å¾Œä¸€æ¬¡æ›´æ–°æ™‚é–“
  - æ›´æ–°ç¶²å€ï¼šç”¨æ–¼è‡ªå‹•æ›´æ–°åµæ¸¬
  
  --------------------------------------------------
  âš  æ³¨æ„äº‹é …
  --------------------------------------------------
  - è«‹å‹¿æ‰‹å‹•æ›´å‹• ID é †åº
  - æ¸…ç©ºè¡¨æ ¼å¾Œå„²å­˜æœƒåŒæ­¥æ¸…ç©º Google Sheet
  - æ›´æ–°åµæ¸¬åƒ…æ”¯æ´å·²å¯¦ä½œ parser çš„ç¶²ç«™
  
  --------------------------------------------------
  ğŸ›  æŠ€è¡“æ¶æ§‹
  --------------------------------------------------
  - å‰ç«¯ä»‹é¢ï¼šGradio
  - è³‡æ–™å„²å­˜ï¼šGoogle Sheet
  - è³‡æ–™è™•ç†ï¼šPandas
  - ç¶²é è§£æï¼šRequests / BeautifulSoup
  """
